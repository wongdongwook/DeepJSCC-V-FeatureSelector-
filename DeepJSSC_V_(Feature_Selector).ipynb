{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPv4SiG/iWyUdXP5xlHrjSe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wongdongwook/DeepJSCC-V-FeatureSelector-/blob/main/DeepJSSC_V_(Feature_Selector).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 코랩 마운트\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVnqWu0-_wfL",
        "outputId": "807c9ac2-846b-4c62-b922-14e15a900a4a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "IPf7WuhaYEg5"
      },
      "outputs": [],
      "source": [
        "#GDN\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "# from torchvision import datasets, transforms\n",
        "# from torchvision.utils import save_image\n",
        "from torch.autograd import Function\n",
        "\n",
        "\n",
        "class LowerBound(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, inputs, bound):\n",
        "        b = torch.ones_like(inputs) * bound\n",
        "        ctx.save_for_backward(inputs, b)\n",
        "        return torch.max(inputs, b)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        inputs, b = ctx.saved_tensors\n",
        "        pass_through_1 = inputs >= b\n",
        "        pass_through_2 = grad_output < 0\n",
        "\n",
        "        pass_through = pass_through_1 | pass_through_2\n",
        "        return pass_through.type(grad_output.dtype) * grad_output, None\n",
        "\n",
        "\n",
        "class GDN(nn.Module):\n",
        "    \"\"\"Generalized divisive normalization layer.\n",
        "    y[i] = x[i] / sqrt(beta[i] + sum_j(gamma[j, i] * x[j]))\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 ch,\n",
        "                 inverse=False,\n",
        "                 beta_min=1e-6,\n",
        "                 gamma_init=0.1,\n",
        "                 reparam_offset=2**-18):\n",
        "        super(GDN, self).__init__()\n",
        "        self.inverse = inverse\n",
        "        self.beta_min = beta_min\n",
        "        self.gamma_init = gamma_init\n",
        "        self.reparam_offset = reparam_offset\n",
        "\n",
        "        self.build(ch)\n",
        "\n",
        "    def build(self, ch):\n",
        "        self.pedestal = self.reparam_offset**2\n",
        "        self.beta_bound = ((self.beta_min + self.reparam_offset**2)**0.5)\n",
        "        self.gamma_bound = self.reparam_offset\n",
        "\n",
        "        # Create beta param\n",
        "        beta = torch.sqrt(torch.ones(ch)+self.pedestal)\n",
        "        self.beta = nn.Parameter(beta)\n",
        "\n",
        "        # Create gamma param\n",
        "        eye = torch.eye(ch)\n",
        "        g = self.gamma_init*eye\n",
        "        g = g + self.pedestal\n",
        "        gamma = torch.sqrt(g)\n",
        "\n",
        "        self.gamma = nn.Parameter(gamma)\n",
        "        self.pedestal = self.pedestal\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        unfold = False\n",
        "        if inputs.dim() == 5:\n",
        "            unfold = True\n",
        "            bs, ch, d, w, h = inputs.size()\n",
        "            inputs = inputs.view(bs, ch, d*w, h)\n",
        "\n",
        "        _, ch, _, _ = inputs.size()\n",
        "\n",
        "        # Beta bound and reparam\n",
        "        beta = LowerBound.apply(self.beta, self.beta_bound)\n",
        "        beta = beta**2 - self.pedestal\n",
        "\n",
        "        # Gamma bound and reparam\n",
        "        gamma = LowerBound.apply(self.gamma, self.gamma_bound)\n",
        "        gamma = gamma**2 - self.pedestal\n",
        "        gamma = gamma.view(ch, ch, 1, 1)\n",
        "\n",
        "        # Norm pool calc\n",
        "        norm_ = nn.functional.conv2d(inputs**2, gamma, beta)\n",
        "        norm_ = torch.sqrt(norm_)\n",
        "\n",
        "        # Apply norm\n",
        "        if self.inverse:\n",
        "            outputs = inputs * norm_\n",
        "        else:\n",
        "            outputs = inputs / norm_\n",
        "\n",
        "        if unfold:\n",
        "            outputs = outputs.view(bs, ch, d, w, h)\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# models\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "def conv(in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
        "    return nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n",
        "\n",
        "def deconv(in_channels, out_channels, kernel_size=3, stride=1, padding=1, output_padding = 0):\n",
        "    return nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, output_padding = output_padding,bias=False)\n",
        "\n",
        "\n",
        "class conv_block(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
        "        super(conv_block, self).__init__()\n",
        "        self.conv = conv(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "        self.gdn = nn.GDN(out_channels)\n",
        "        self.prelu = nn.PReLU()\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        out = self.gdn(out)\n",
        "        out = self.prelu(out)\n",
        "        return out\n",
        "\n",
        "class deconv_block(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, output_padding = 0):\n",
        "        super(deconv_block, self).__init__()\n",
        "        self.deconv = deconv(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding,  output_padding = output_padding)\n",
        "        self.gdn = nn.GDN(out_channels)\n",
        "        self.prelu = nn.PReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    def forward(self, x, activate_func='prelu'):\n",
        "        out = self.deconv(x)\n",
        "        out = self.gdn(out)\n",
        "        if activate_func=='prelu':\n",
        "            out = self.prelu(out)\n",
        "        elif activate_func=='sigmoid':\n",
        "            out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class conv_ResBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, use_conv1x1=False, kernel_size=3, stride=1, padding=1):\n",
        "        super(conv_ResBlock, self).__init__()\n",
        "        self.conv1 = conv(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "        self.conv2 = conv(out_channels, out_channels, kernel_size=1, stride = 1, padding=0)\n",
        "        self.gdn1 = GDN(out_channels)\n",
        "        self.gdn2 = GDN(out_channels)\n",
        "        self.prelu = nn.PReLU()\n",
        "        self.use_conv1x1 = use_conv1x1\n",
        "        if use_conv1x1 == True:\n",
        "            self.conv3 = conv(in_channels, out_channels, kernel_size=1, stride=stride, padding=0)\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.gdn1(out)\n",
        "        out = self.prelu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.gdn2(out)\n",
        "        if self.use_conv1x1 == True:\n",
        "            x = self.conv3(x)\n",
        "        out = out+x\n",
        "        out = self.prelu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class deconv_ResBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, use_deconv1x1=False, kernel_size=3, stride=1, padding=1, output_padding=0):\n",
        "        super(deconv_ResBlock, self).__init__()\n",
        "        self.deconv1 = deconv(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, output_padding=output_padding)\n",
        "        self.deconv2 = deconv(out_channels, out_channels, kernel_size=1, stride = 1, padding=0, output_padding=0)\n",
        "        self.gdn1 = GDN(out_channels)\n",
        "        self.gdn2 = GDN(out_channels)\n",
        "        self.prelu = nn.PReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.use_deconv1x1 = use_deconv1x1\n",
        "        if use_deconv1x1 == True:\n",
        "            self.deconv3 = deconv(in_channels, out_channels, kernel_size=1, stride=stride, padding=0, output_padding=output_padding)\n",
        "    def forward(self, x, activate_func='prelu'):\n",
        "        out = self.deconv1(x)\n",
        "        out = self.gdn1(out)\n",
        "        out = self.prelu(out)\n",
        "        out = self.deconv2(out)\n",
        "        out = self.gdn2(out)\n",
        "        if self.use_deconv1x1 == True:\n",
        "            x = self.deconv3(x)\n",
        "        out = out+x\n",
        "        if activate_func=='prelu':\n",
        "            out = self.prelu(out)\n",
        "        elif activate_func=='sigmoid':\n",
        "            out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "# Original Existing Works\n",
        "class AF_block(nn.Module):\n",
        "    def __init__(self, Nin, Nh, No):\n",
        "        super(AF_block, self).__init__()\n",
        "        self.fc1 = nn.Linear(Nin+1, Nh)\n",
        "        self.fc2 = nn.Linear(Nh, No)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    def forward(self, x, snr):\n",
        "        # out = F.adaptive_avg_pool2d(x, (1,1))\n",
        "        # out = torch.squeeze(out)\n",
        "        # out = torch.cat((out, snr), 1)\n",
        "        if snr.shape[0]>1:\n",
        "            snr = snr.squeeze()\n",
        "\n",
        "        snr = snr.unsqueeze(1)\n",
        "        mu = torch.mean(x, (2, 3))\n",
        "        out = torch.cat((mu, snr), 1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.sigmoid(out)\n",
        "        out = out.unsqueeze(2)\n",
        "        out = out.unsqueeze(3)\n",
        "        out = out*x\n",
        "        return out\n",
        "\n",
        "# The Encoder model with attention feature blocks\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, enc_shape, kernel_sz, Nc_conv):\n",
        "        super(Encoder, self).__init__()\n",
        "        enc_N = enc_shape[0]\n",
        "        Nh_AF = Nc_conv//2\n",
        "        padding_L = (kernel_sz-1)//2\n",
        "        self.conv1 = conv_ResBlock(3, Nc_conv, use_conv1x1=True, kernel_size = kernel_sz, stride = 2, padding=padding_L) # 맨 첫번째인자가 데이터셋의 채널수에따라서 1이 3으로 바뀌고 3이 1로 바뀔 수 있음\n",
        "        self.conv2 = conv_ResBlock(Nc_conv, Nc_conv, use_conv1x1=True, kernel_size = kernel_sz, stride = 2, padding=padding_L)\n",
        "        self.conv3 = conv_ResBlock(Nc_conv, Nc_conv, kernel_size = kernel_sz, stride = 1, padding=padding_L)\n",
        "        self.conv4 = conv_ResBlock(Nc_conv, Nc_conv, kernel_size = kernel_sz, stride = 1, padding=padding_L)\n",
        "        self.conv5 = conv_ResBlock(Nc_conv, enc_N, use_conv1x1=True, kernel_size = kernel_sz, stride = 1, padding=padding_L)\n",
        "        self.AF1 = AF_block(Nc_conv, Nh_AF, Nc_conv)\n",
        "        self.AF2 = AF_block(Nc_conv, Nh_AF, Nc_conv)\n",
        "        self.AF3 = AF_block(Nc_conv, Nh_AF, Nc_conv)\n",
        "        self.AF4 = AF_block(Nc_conv, Nh_AF, Nc_conv)\n",
        "        self.AF5 = AF_block(enc_N, enc_N//2, enc_N)\n",
        "        self.flatten = nn.Flatten()\n",
        "    def forward(self, x, snr):\n",
        "        #snr = snr.view(-1, 1)\n",
        "        out = self.conv1(x)\n",
        "        out = self.AF1(out, snr)\n",
        "        out = self.conv2(out)\n",
        "        out = self.AF2(out, snr)\n",
        "        out = self.conv3(out)\n",
        "        out = self.AF3(out, snr)\n",
        "        out = self.conv4(out)\n",
        "        out = self.AF4(out, snr)\n",
        "        out = self.conv5(out)\n",
        "        out = self.AF5(out, snr)\n",
        "        out = self.flatten(out)\n",
        "        return out\n",
        "\n",
        "# The Decoder model with attention feature blocks\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, enc_shape, kernel_sz, Nc_deconv):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.enc_shape = enc_shape\n",
        "        Nh_AF1 = enc_shape[0]//2\n",
        "        Nh_AF = Nc_deconv//2\n",
        "        padding_L = (kernel_sz-1)//2\n",
        "        self.deconv1 = deconv_ResBlock(self.enc_shape[0], Nc_deconv, use_deconv1x1=True, kernel_size = kernel_sz, stride = 2,  padding=padding_L, output_padding = 1)\n",
        "        self.deconv2 = deconv_ResBlock(Nc_deconv, Nc_deconv, use_deconv1x1=True, kernel_size = kernel_sz, stride = 2,  padding=padding_L, output_padding = 1)\n",
        "        self.deconv3 = deconv_ResBlock(Nc_deconv, Nc_deconv, kernel_size=kernel_sz, stride=1, padding=padding_L)\n",
        "        self.deconv4 = deconv_ResBlock(Nc_deconv, Nc_deconv, kernel_size=kernel_sz, stride=1, padding=padding_L)\n",
        "        self.deconv5 = deconv_ResBlock(Nc_deconv, 3, use_deconv1x1=True, kernel_size=kernel_sz, stride=1, padding=padding_L)\n",
        "\n",
        "        self.AF1 = AF_block(self.enc_shape[0], Nh_AF1, self.enc_shape[0])\n",
        "        self.AF2 = AF_block(Nc_deconv, Nh_AF, Nc_deconv)\n",
        "        self.AF3 = AF_block(Nc_deconv, Nh_AF, Nc_deconv)\n",
        "        self.AF4 = AF_block(Nc_deconv, Nh_AF, Nc_deconv)\n",
        "        self.AF5 = AF_block(Nc_deconv, Nh_AF, Nc_deconv)\n",
        "    def forward(self, x, snr):\n",
        "        #snr = snr.view(-1, 1)\n",
        "        out = x.view(-1, self.enc_shape[0], self.enc_shape[1], self.enc_shape[2])\n",
        "        out = self.AF1(out, snr)\n",
        "        out = self.deconv1(out)\n",
        "        out = self.AF2(out, snr)\n",
        "        out = self.deconv2(out)\n",
        "        out = self.AF3(out, snr)\n",
        "        out = self.deconv3(out)\n",
        "        out = self.AF4(out, snr)\n",
        "        out = self.deconv4(out)\n",
        "        out = self.AF5(out, snr)\n",
        "        out = self.deconv5(out, 'sigmoid')\n",
        "        return out\n",
        "\n",
        "# Power normalization before transmission\n",
        "# Note: if P = 1, the symbol power is 2\n",
        "# If you want to set the average power as 1, please change P as P=1/np.sqrt(2)\n",
        "def Power_norm(z, P = 1):\n",
        "    batch_size, z_dim = z.shape\n",
        "    z_power = torch.sqrt(torch.sum(z**2, 1))\n",
        "    z_M = z_power.repeat(z_dim, 1)\n",
        "    return np.sqrt(P*z_dim)*z/z_M.t()\n",
        "\n",
        "def Power_norm_complex(z, P = 1):\n",
        "    batch_size, z_dim = z.shape\n",
        "    z_com = torch.complex(z[:, 0:z_dim:2], z[:, 1:z_dim:2])\n",
        "    z_com_conj = torch.complex(z[:, 0:z_dim:2], -z[:, 1:z_dim:2])\n",
        "    z_power = torch.sum(z_com*z_com_conj, 1).real\n",
        "    z_M = z_power.repeat(z_dim//2, 1)\n",
        "    z_nlz = np.sqrt(P*z_dim)*z_com/torch.sqrt(z_M.t())\n",
        "    z_out = torch.zeros(batch_size, z_dim).cuda()\n",
        "    z_out[:, 0:z_dim:2] = z_nlz.real\n",
        "    z_out[:, 1:z_dim:2] = z_nlz.imag\n",
        "    return z_out\n",
        "\n",
        "# The (real) AWGN channel\n",
        "def AWGN_channel(x, snr, P = 2):\n",
        "    batch_size, length = x.shape\n",
        "    gamma = 10 ** (snr / 10.0)\n",
        "    noise = torch.sqrt(P/gamma)*torch.randn(batch_size, length).cuda()\n",
        "    y = x+noise\n",
        "    return y\n",
        "\n",
        "def AWGN_complex(x, snr, Ps = 1):\n",
        "    batch_size, length = x.shape\n",
        "    gamma = 10 ** (snr / 10.0)\n",
        "    n_I = torch.sqrt(Ps/gamma)*torch.randn(batch_size, length).cuda()\n",
        "    n_R = torch.sqrt(Ps/gamma)*torch.randn(batch_size, length).cuda()\n",
        "    noise = torch.complex(n_I, n_R)\n",
        "    y = x + noise\n",
        "    return y\n",
        "\n",
        "# Please set the symbol power if it is not a default value\n",
        "def Fading_channel(x, snr, P = 2):\n",
        "    gamma = 10 ** (snr / 10.0)\n",
        "    [batch_size, feature_length] = x.shape\n",
        "    K = feature_length//2\n",
        "\n",
        "    h_I = torch.randn(batch_size, K).cuda()\n",
        "    h_R = torch.randn(batch_size, K).cuda()\n",
        "    h_com = torch.complex(h_I, h_R)\n",
        "    x_com = torch.complex(x[:, 0:feature_length:2], x[:, 1:feature_length:2])\n",
        "    y_com = h_com*x_com\n",
        "\n",
        "    n_I = torch.sqrt(P/gamma)*torch.randn(batch_size, K).cuda()\n",
        "    n_R = torch.sqrt(P/gamma)*torch.randn(batch_size, K).cuda()\n",
        "    noise = torch.complex(n_I, n_R)\n",
        "\n",
        "    y_add = y_com + noise\n",
        "    y = y_add/h_com\n",
        "\n",
        "    y_out = torch.zeros(batch_size, feature_length).cuda()\n",
        "    y_out[:, 0:feature_length:2] = y.real\n",
        "    y_out[:, 1:feature_length:2] = y.imag\n",
        "    return y_out\n",
        "\n",
        "\n",
        "\n",
        "# Note: if P = 1, the symbol power is 2\n",
        "# If you want to set the average power as 1, please change P as P=1/np.sqrt(2)\n",
        "def Power_norm_VLC(z, cr, P = 1):\n",
        "    batch_size, z_dim = z.shape\n",
        "    Kv = torch.ceil(z_dim*cr).int()\n",
        "    z_power = torch.sqrt(torch.sum(z**2, 1))\n",
        "    z_M = z_power.repeat(z_dim, 1).cuda()\n",
        "    return torch.sqrt(Kv*P)*z/z_M.t()\n",
        "\n",
        "\n",
        "def AWGN_channel_VLC(x, snr, cr, P = 2):\n",
        "    batch_size, length = x.shape\n",
        "    gamma = 10 ** (snr / 10.0)\n",
        "    mask = mask_gen(length, cr).cuda()\n",
        "    noise = torch.sqrt(P/gamma)*torch.randn(1, length).cuda()\n",
        "    noise = noise*mask\n",
        "    y = x+noise\n",
        "    return y\n",
        "\n",
        "\n",
        "def Fading_channel_VLC(x, snr, cr, P = 2):\n",
        "    gamma = 10 ** (snr / 10.0)\n",
        "    [batch_size, feature_length] = x.shape\n",
        "    K = feature_length//2\n",
        "\n",
        "    mask = mask_gen(K, cr).cuda()\n",
        "    h_I = torch.randn(batch_size, K).cuda()\n",
        "    h_R = torch.randn(batch_size, K).cuda()\n",
        "    h_com = torch.complex(h_I, h_R)\n",
        "    x_com = torch.complex(x[:, 0:feature_length:2], x[:, 1:feature_length:2])\n",
        "    y_com = h_com*x_com\n",
        "\n",
        "    n_I = torch.sqrt(P/gamma)*torch.randn(batch_size, K).cuda()\n",
        "    n_R = torch.sqrt(P/gamma)*torch.randn(batch_size, K).cuda()\n",
        "    noise = torch.complex(n_I, n_R)*mask\n",
        "\n",
        "    y_add = y_com + noise\n",
        "    y = y_add/h_com\n",
        "\n",
        "    y_out = torch.zeros(batch_size, feature_length).cuda()\n",
        "    y_out[:, 0:feature_length:2] = y.real\n",
        "    y_out[:, 1:feature_length:2] = y.imag\n",
        "    return y_out\n",
        "\n",
        "\n",
        "def Channel(z, snr, channel_type = 'AWGN'):\n",
        "    z = Power_norm(z)\n",
        "    if channel_type == 'AWGN':\n",
        "        z = AWGN_channel(z, snr)\n",
        "    elif channel_type == 'Fading':\n",
        "        z = Fading_channel(z, snr)\n",
        "    return z\n",
        "\n",
        "\n",
        "def Channel_VLC(z, snr, cr, channel_type = 'AWGN'):\n",
        "    z = Power_norm_VLC(z, cr)\n",
        "    if channel_type == 'AWGN':\n",
        "        z = AWGN_channel_VLC(z, snr, cr)\n",
        "    elif channel_type == 'Fading':\n",
        "        z = Fading_channel_VLC(z, snr, cr)\n",
        "    return z\n",
        "\n",
        "\n",
        "def mask_gen(N, cr, ch_max = 48):\n",
        "    MASK = torch.zeros(cr.shape[0], N).int()\n",
        "    nc = N//ch_max\n",
        "    for i in range(0, cr.shape[0]):\n",
        "        L_i = nc*torch.round(ch_max*cr[i]).int()\n",
        "        MASK[i, 0:L_i] = 1\n",
        "    return MASK\n",
        "\n",
        "\n",
        "def mask_gen_manual(N, cr, binary_mask_vector, ch_max = 48):\n",
        "    \"\"\"\n",
        "    기존 mask_gen 함수 수정 버전.\n",
        "    - 입력된 binary_mask_vector에 따라 마스킹 설정함.\n",
        "\n",
        "    Args:\n",
        "        N: 총 feature 개수\n",
        "        cr: (batch_size,) compression ratio (기존과 동일하나 지금은 실제 사용 안함)\n",
        "        binary_mask_vector: (batch_size, N) 크기의 binary mask 입력 (0 또는 1)\n",
        "        ch_max: 최대 채널 수 (기존과 동일하나 지금은 실제 사용 안함)\n",
        "\n",
        "    Returns:\n",
        "        MASK: (batch_size, N) 크기의 torch.Tensor\n",
        "    \"\"\"\n",
        "    MASK = torch.zeros(cr.shape[0], N).int()\n",
        "    assert binary_mask_vector.shape[1] == N, \"binary_mask_vector shape mismatch with N\"\n",
        "    MASK = binary_mask_vector[:, :N].int().clone()  # 안전하게 int 타입 복사\n",
        "    return MASK\n",
        "\n",
        "\n",
        "class ADJSCC(nn.Module):\n",
        "    def __init__(self, enc_shape, Kernel_sz, Nc):\n",
        "        super(ADJSCC, self).__init__()\n",
        "        self.encoder = Encoder(enc_shape, Kernel_sz, Nc)\n",
        "        self.decoder = Decoder(enc_shape, Kernel_sz, Nc)\n",
        "    def forward(self, x, snr, channel_type = 'AWGN'):\n",
        "        z = self.encoder(x, snr)\n",
        "        z = Channel(z, snr, channel_type)\n",
        "        out = self.decoder(z, snr)\n",
        "        return out\n",
        "\n",
        "class ADJSCC_V(nn.Module):\n",
        "    def __init__(self, enc_shape, Kernel_sz, Nc):\n",
        "        super(ADJSCC_V, self).__init__()\n",
        "        self.encoder = Encoder(enc_shape, Kernel_sz, Nc)\n",
        "        self.decoder = Decoder(enc_shape, Kernel_sz, Nc)\n",
        "\n",
        "    def forward(self, x, snr, cr, binary_mask_vector, channel_type='AWGN'):\n",
        "        z = self.encoder(x, snr)\n",
        "        z = z * mask_gen_manual(z.shape[1], cr, binary_mask_vector).cuda()\n",
        "        z = Channel_VLC(z, snr, cr, channel_type)\n",
        "        out = self.decoder(z, snr)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "uC5dXz8ZYihk"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import cifar100\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# CIFAR-100 Data Loading Function\n",
        "def load_cifar100_data():\n",
        "    (x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
        "    x_train = np.transpose(x_train, (0, 3, 1, 2))  # Dimension Rearragentment: [batch size, channel, height, width]\n",
        "    x_test = np.transpose(x_test, (0, 3, 1, 2))    # Dimension Rearragentment\n",
        "    x_train = x_train.astype('float32') / 255\n",
        "    x_test = x_test.astype('float32') / 255\n",
        "    return x_train, x_test\n",
        "\n",
        "# Data Loader 클래스\n",
        "class DatasetFolder(Dataset):\n",
        "    def __init__(self, matData):\n",
        "        self.matdata = matData\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.matdata[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.matdata.shape[0]"
      ],
      "metadata": {
        "id": "M2v13rJ8Yj9I"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 400\n",
        "LEARNING_RATE = 1e-4\n",
        "#LEARNING_RATE = 3e-4\n",
        "PRINT_RREQ = 150\n",
        "\n",
        "CHANNEL = 'AWGN'  # Choose AWGN or Fading\n",
        "IMG_SIZE = [3, 32, 32]  # CIFAR-100 Image shape\n",
        "N_channels = 256\n",
        "Kernel_sz = 5\n",
        "\n",
        "x_train, x_test = load_cifar100_data()\n",
        "\n",
        "train_dataset = DatasetFolder(x_train)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
        "test_dataset = DatasetFolder(x_test)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "current_epoch = 0\n",
        "CONTINUE_TRAINING = False\n",
        "\n",
        "KSZ = str(Kernel_sz)+'x'+str(Kernel_sz)+'_'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "989kJeJZYkAF",
        "outputId": "302015a6-55bb-4b2f-9dd1-6a7a1fada22a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "\u001b[1m169001437/169001437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test Metric (PSNR)\n",
        "from skimage.metrics import peak_signal_noise_ratio as compute_pnsr\n",
        "\n",
        "def Img_transform(test_rec):\n",
        "    test_rec = test_rec.permute(0, 2, 3, 1)\n",
        "    test_rec = test_rec.cpu().detach().numpy()\n",
        "    test_rec = test_rec*255\n",
        "    test_rec = test_rec.astype(np.uint8)\n",
        "    return test_rec\n",
        "\n",
        "def Compute_batch_PSNR(test_input, test_rec):\n",
        "    psnr_i1 = np.zeros((test_input.shape[0]))\n",
        "    for j in range(0, test_input.shape[0]):\n",
        "        psnr_i1[j] = compute_pnsr(test_input[j, :], test_rec[j, :])\n",
        "    psnr_ave = np.mean(psnr_i1)\n",
        "    return psnr_ave\n",
        "\n",
        "\n",
        "def Compute_IMG_PSNR(test_input, test_rec):\n",
        "    psnr_i1 = np.zeros((test_input.shape[0], 1))\n",
        "    for j in range(0, test_input.shape[0]):\n",
        "        psnr_i1[j] = compute_pnsr(test_input[j, :], test_rec[j, :])\n",
        "    return psnr_i1\n"
      ],
      "metadata": {
        "id": "-eGPDfqSYmu6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LEARNING_RATE = 1e-4\n",
        "CONTINUE_TRAINING= True\n",
        "enc_out_shape = [48, IMG_SIZE[1]//4, IMG_SIZE[2]//4] #가장 첫번째가 48,  오른쪽이 feature map // ADJSCC인 경우 수정되어야함\n",
        "\n",
        "DeepJSCC_V = ADJSCC_V(enc_out_shape, Kernel_sz, N_channels).cuda()\n",
        "criterion = nn.MSELoss().cuda()\n",
        "optimizer = torch.optim.Adam(DeepJSCC_V.parameters(), lr=LEARNING_RATE)\n",
        "weight_path = \"/content/drive/MyDrive/DeepJSCC-V_FS_weights/DeepJSCC_5x5_AWGN_256_20.pth.tar\"\n",
        "\n",
        "current_epoch = 0\n",
        "bestLoss = 1e3\n",
        "if CONTINUE_TRAINING == True:\n",
        "    #DeepJSCC_V.load_state_dict(torch.load('./JSCC_models/DeepJSCC_VLC_'+KSZ+CHANNEL+'_'+str(N_channels)+'_201.pth.tar')['state_dict'])\n",
        "    #current_epoch = 204\n",
        "    current_epoch = 10\n",
        "    DeepJSCC_V.load_state_dict(torch.load(weight_path)['state_dict'])\n"
      ],
      "metadata": {
        "id": "XZ4K8AcpYoIr"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DeepJSCC_V + Exisitng Attention\n",
        "print('Training for DeepJSCC_V is started!')\n",
        "bestLoss = 1e3\n",
        "\n",
        "current_epoch=0\n",
        "EPOCHS= 101\n",
        "\n",
        "for epoch in range(current_epoch, EPOCHS):\n",
        "    DeepJSCC_V.train()\n",
        "    print('========================')\n",
        "    print('lr:%.4e'%optimizer.param_groups[0]['lr'])\n",
        "\n",
        "    # Model training\n",
        "    for i, x_input in enumerate(train_loader):\n",
        "        x_input = x_input.cuda()\n",
        "        SNR_TRAIN = torch.randint(0, 28, (x_input.shape[0], 1)).cuda()\n",
        "        CR = 0.1+0.9*torch.rand(x_input.shape[0], 1).cuda()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            dummy_z = DeepJSCC_V.encoder(x_input, SNR_TRAIN)\n",
        "            N = dummy_z.shape[1]\n",
        "\n",
        "        # binary_mask_vector 초기화\n",
        "        binary_mask_vector = torch.zeros(x_input.shape[0], N).int().cuda()\n",
        "\n",
        "        # 각 샘플마다 N * CR[i] 개수만큼 1을 랜덤 위치에 할당\n",
        "        for i in range(x_input.shape[0]):\n",
        "            k = int(torch.round(N * CR[i]).item())  # 선택할 feature 수\n",
        "            indices = torch.randperm(N)[:k]  # 무작위로 k개 인덱스 선택\n",
        "            binary_mask_vector[i, indices] = 1\n",
        "\n",
        "        # ✅ 모델 forward\n",
        "        x_rec = DeepJSCC_V(x_input, SNR_TRAIN, CR, binary_mask_vector, CHANNEL)\n",
        "\n",
        "        loss = criterion(x_input, x_rec)\n",
        "        loss = loss.mean()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if i % PRINT_RREQ == 0:\n",
        "            print('Epoch: [{0}][{1}/{2}]\\t' 'Loss {loss:.4f}\\t'.format(epoch, i, len(train_loader), loss=loss.item()))\n",
        "\n",
        "    # Model Evaluation\n",
        "    # Model Evaluation\n",
        "    DeepJSCC_V.eval()\n",
        "    totalLoss = 0\n",
        "    with torch.no_grad():\n",
        "        for i, test_input in enumerate(test_loader):\n",
        "            test_input = test_input.cuda()\n",
        "            SNR_TEST = torch.randint(0, 28, (test_input.shape[0], 1)).cuda()\n",
        "            CR = 0.1 + 0.9 * torch.rand(test_input.shape[0], 1).cuda()\n",
        "\n",
        "            # ✅ 정확한 feature 차원 N 계산\n",
        "            dummy_z = DeepJSCC_V.encoder(test_input, SNR_TEST)\n",
        "            N = dummy_z.shape[1]\n",
        "\n",
        "            # ✅ binary_mask_vector 생성 (N * CR[i] 개수만큼 1 설정)\n",
        "            binary_mask_vector = torch.zeros(test_input.shape[0], N).int().cuda()\n",
        "            for j in range(test_input.shape[0]):\n",
        "                k = int(torch.round(N * CR[j]).item())\n",
        "                indices = torch.randperm(N)[:k]\n",
        "                binary_mask_vector[j, indices] = 1\n",
        "\n",
        "            # ✅ 모델 forward with binary mask\n",
        "            test_rec = DeepJSCC_V(test_input, SNR_TEST, CR, binary_mask_vector, CHANNEL)\n",
        "\n",
        "            totalLoss += criterion(test_rec, test_input).item() * test_input.size(0)\n",
        "\n",
        "        averageLoss = totalLoss / len(test_dataset)\n",
        "        print('averageLoss =', averageLoss)\n",
        "\n",
        "        if averageLoss < bestLoss:\n",
        "            if not os.path.exists('./JSCC_models'):\n",
        "                os.makedirs('./JSCC_models')\n",
        "            torch.save({'state_dict': DeepJSCC_V.state_dict()},\n",
        "                      './JSCC_models/DeepJSCC_' + KSZ + CHANNEL + '_' + str(N_channels) + '_' + str(epoch) + '.pth.tar')\n",
        "\n",
        "            print('Model saved')\n",
        "            bestLoss = averageLoss\n",
        "\n",
        "\n",
        "print('Training for DeepJSCC_V is finished!')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ftoKeTRoYpHP",
        "outputId": "581b85aa-6572-415e-e087-f65e20455da3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for DeepJSCC_V is started!\n",
            "========================\n",
            "lr:1.0000e-04\n",
            "averageLoss = 0.011643200239539147\n",
            "Model saved\n",
            "========================\n",
            "lr:1.0000e-04\n",
            "averageLoss = 0.009639013962447643\n",
            "Model saved\n",
            "========================\n",
            "lr:1.0000e-04\n",
            "averageLoss = 0.007487355737388134\n",
            "Model saved\n",
            "========================\n",
            "lr:1.0000e-04\n",
            "averageLoss = 0.007030338482558727\n",
            "Model saved\n",
            "========================\n",
            "lr:1.0000e-04\n",
            "averageLoss = 0.006136258731782436\n",
            "Model saved\n",
            "========================\n",
            "lr:1.0000e-04\n",
            "averageLoss = 0.005899026136100292\n",
            "Model saved\n",
            "========================\n",
            "lr:1.0000e-04\n",
            "averageLoss = 0.005690532514452934\n",
            "Model saved\n",
            "========================\n",
            "lr:1.0000e-04\n",
            "averageLoss = 0.005361617161333561\n",
            "Model saved\n",
            "========================\n",
            "lr:1.0000e-04\n",
            "averageLoss = 0.005173006188124418\n",
            "Model saved\n",
            "========================\n",
            "lr:1.0000e-04\n",
            "averageLoss = 0.005034783643484116\n",
            "Model saved\n",
            "========================\n",
            "lr:1.0000e-04\n",
            "averageLoss = 0.0048060332141816615\n",
            "Model saved\n",
            "========================\n",
            "lr:1.0000e-04\n",
            "averageLoss = 0.0049720487996935845\n",
            "========================\n",
            "lr:1.0000e-04\n",
            "averageLoss = 0.0046873282879590986\n",
            "Model saved\n",
            "========================\n",
            "lr:1.0000e-04\n",
            "averageLoss = 0.0044936185084283355\n",
            "Model saved\n",
            "========================\n",
            "lr:1.0000e-04\n",
            "averageLoss = 0.004688002010434866\n",
            "========================\n",
            "lr:1.0000e-04\n",
            "averageLoss = 0.004271629337966442\n",
            "Model saved\n",
            "========================\n",
            "lr:1.0000e-04\n",
            "averageLoss = 0.004411960044503212\n",
            "========================\n",
            "lr:1.0000e-04\n",
            "averageLoss = 0.004203038689494133\n",
            "Model saved\n",
            "========================\n",
            "lr:1.0000e-04\n",
            "averageLoss = 0.0042162998497486115\n",
            "========================\n",
            "lr:1.0000e-04\n",
            "averageLoss = 0.003956704151630402\n",
            "Model saved\n",
            "========================\n",
            "lr:1.0000e-04\n",
            "averageLoss = 0.0038803845062851906\n",
            "Model saved\n",
            "========================\n",
            "lr:1.0000e-04\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-f94e318e919c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Model training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_input\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mx_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mSNR_TRAIN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mCR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kernel_sz = 5\n",
        "KSZ = '_' + str(kernel_sz) + 'x' + str(kernel_sz) + '_'\n",
        "PSNR_ave = np.zeros((10, 10))\n",
        "cr = 1 / 3  # fixed test compression ratio\n",
        "\n",
        "for m in range(0, 10):\n",
        "    for k in range(0, 10):\n",
        "        print(f'Evaluating DeepJSCC-v with CR = {cr:.2f} and SNR = {3*k - 3} dB')\n",
        "        total_psnr = 0\n",
        "        DeepJSCC_V.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, test_input in enumerate(test_loader):\n",
        "                test_input = test_input.cuda()\n",
        "                SNR = 3 * (k - 1) * torch.ones((test_input.shape[0], 1)).cuda()\n",
        "                CR = cr * torch.ones((test_input.shape[0], 1)).cuda()\n",
        "\n",
        "                # ✅ feature 수 N 계산\n",
        "                dummy_z = DeepJSCC_V.encoder(test_input, SNR)\n",
        "                N = dummy_z.shape[1]\n",
        "\n",
        "                # ✅ binary_mask_vector 생성\n",
        "                binary_mask_vector = torch.zeros(test_input.shape[0], N).int().cuda()\n",
        "                for j in range(test_input.shape[0]):\n",
        "                    k_feat = int(torch.round(N * CR[j]).item())\n",
        "                    indices = torch.randperm(N)[:k_feat]\n",
        "                    binary_mask_vector[j, indices] = 1\n",
        "\n",
        "                # ✅ forward 호출\n",
        "                test_rec = DeepJSCC_V(test_input, SNR, CR, binary_mask_vector, CHANNEL)\n",
        "\n",
        "                test_input = Img_transform(test_input)\n",
        "                test_rec = Img_transform(test_rec)\n",
        "                psnr_ave = Compute_batch_PSNR(test_input, test_rec)\n",
        "                total_psnr += psnr_ave\n",
        "\n",
        "            averagePSNR = total_psnr / i\n",
        "            print('PSNR =', averagePSNR)\n",
        "\n",
        "        PSNR_ave[m, k] = averagePSNR\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "id": "W3bkKDabYqK_",
        "outputId": "f9a03a6d-59e9-4d4d-b250-dccb132f8496"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating DeepJSCC-v with CR = 0.33 and SNR = -3 dB\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-63feb9d65a1e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mtest_rec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeepJSCC_V\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSNR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary_mask_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCHANNEL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                 \u001b[0mtest_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImg_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m                 \u001b[0mtest_rec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImg_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_rec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mpsnr_ave\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCompute_batch_PSNR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_rec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-2758fa518a07>\u001b[0m in \u001b[0;36mImg_transform\u001b[0;34m(test_rec)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mImg_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_rec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtest_rec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_rec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtest_rec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_rec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mtest_rec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_rec\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtest_rec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_rec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "8QK3b1SaB5VI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def compute_feature_importance_table(model, test_loader, CHANNEL, Img_transform, Compute_batch_PSNR,\n",
        "                                     CR_list=[0.1, 0.3, 0.5, 0.7, 0.9],\n",
        "                                     SNR_list=[0, 6, 12, 18]):\n",
        "    model.eval()\n",
        "    importance_table = None\n",
        "    start_time = time.time()\n",
        "\n",
        "    total_tasks = len(CR_list) * len(SNR_list)\n",
        "    tasks_completed = 0\n",
        "\n",
        "    for cr_idx, cr_val in enumerate(CR_list):\n",
        "        for snr_idx, snr_val in enumerate(SNR_list):\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            for test_input in test_loader:\n",
        "                test_input = test_input.cuda()\n",
        "                snr = snr_val * torch.ones((test_input.shape[0], 1)).cuda()\n",
        "                cr = cr_val * torch.ones((test_input.shape[0], 1)).cuda()\n",
        "\n",
        "                z = model.encoder(test_input, snr).detach()\n",
        "                N = z.shape[1]\n",
        "                if importance_table is None:\n",
        "                    importance_table = torch.zeros(len(CR_list), len(SNR_list), N).cuda()\n",
        "\n",
        "                full_mask = torch.ones_like(z).int()\n",
        "                z_masked = z * full_mask\n",
        "                z_channel = Channel_VLC(z_masked, snr, cr, CHANNEL)\n",
        "                rec_full = model.decoder(z_channel, snr).detach()\n",
        "                psnr_full = Compute_batch_PSNR(Img_transform(test_input), Img_transform(rec_full))\n",
        "\n",
        "                drop_scores = torch.zeros(N).cuda()\n",
        "                for f in range(N):\n",
        "                    iter_start = time.time()\n",
        "                    mask = full_mask.clone()\n",
        "                    mask[:, f] = 0\n",
        "                    z_f = z * mask\n",
        "                    z_channel_f = Channel_VLC(z_f, snr, cr, CHANNEL)\n",
        "                    rec_f = model.decoder(z_channel_f, snr).detach()\n",
        "                    psnr_f = Compute_batch_PSNR(Img_transform(test_input), Img_transform(rec_f))\n",
        "                    drop_scores[f] = psnr_full - psnr_f\n",
        "\n",
        "                    del mask, z_f, z_channel_f, rec_f, psnr_f\n",
        "                    torch.cuda.empty_cache()\n",
        "                    gc.collect()\n",
        "\n",
        "                    # ✅ 중간 출력: feature-level 진행률\n",
        "                    feature_elapsed = time.time() - iter_start\n",
        "                    percent = (f + 1) / N * 100\n",
        "                    print(f\"    - Feature {f+1}/{N} ({percent:.1f}%) 완료 | 소요: {feature_elapsed:.1f}s\")\n",
        "\n",
        "                importance_table[cr_idx, snr_idx, :] = drop_scores\n",
        "\n",
        "                # 시간 측정 및 ETA 출력\n",
        "                tasks_completed += 1\n",
        "                elapsed = time.time() - start_time\n",
        "                avg_per_task = elapsed / tasks_completed\n",
        "                remaining_tasks = total_tasks - tasks_completed\n",
        "                eta = remaining_tasks * avg_per_task\n",
        "                print(f\"[CR={cr_val:.1f}, SNR={snr_val}] 완료 | 경과: {elapsed:.1f}s | ETA: {eta:.1f}s\")\n",
        "\n",
        "                # 선택적 중간 시각화 (주석처리 가능)\n",
        "                show_intermediate_heatmap(\n",
        "                    importance_table[:cr_idx+1, :, :].cpu().numpy(),\n",
        "                    CR_list[:cr_idx+1],\n",
        "                    SNR_list\n",
        "                )\n",
        "\n",
        "                break\n",
        "\n",
        "    return importance_table.cpu().numpy()\n"
      ],
      "metadata": {
        "id": "eVGEL90b8hQk"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_intermediate_heatmap(partial_table, partial_CR_list, SNR_list):\n",
        "    num_cr, num_snr, N = partial_table.shape\n",
        "    for cr_idx, cr_val in enumerate(partial_CR_list):\n",
        "        plt.figure(figsize=(10, 3))\n",
        "        plt.imshow(partial_table[cr_idx], aspect='auto', cmap='plasma')\n",
        "        plt.colorbar(label='PSNR Drop (Feature Importance)')\n",
        "        plt.xticks(np.arange(0, N, step=4), labels=[f'{i}' for i in range(0, N, 4)])\n",
        "        plt.yticks(np.arange(num_snr), labels=[f'SNR={s}' for s in SNR_list])\n",
        "        plt.xlabel('Feature Index')\n",
        "        plt.ylabel('SNR')\n",
        "        plt.title(f'[중간결과] Feature Importance @ CR={cr_val:.1f}')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "IdSL-UquCjXg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_importance_heatmap(importance_table, CR_list, SNR_list):\n",
        "    num_cr, num_snr, N = importance_table.shape\n",
        "    for cr_idx, cr_val in enumerate(CR_list):\n",
        "        plt.figure(figsize=(12, 3))\n",
        "        plt.imshow(importance_table[cr_idx], aspect='auto', cmap='plasma')\n",
        "        plt.colorbar(label='PSNR Drop (Feature Importance)')\n",
        "        plt.xticks(np.arange(0, N, step=4), labels=[f'{i}' for i in range(0, N, 4)])\n",
        "        plt.yticks(np.arange(num_snr), labels=[f'SNR={s}' for s in SNR_list])\n",
        "        plt.xlabel('Feature Index')\n",
        "        plt.ylabel('SNR')\n",
        "        plt.title(f'Feature Importance Heatmap @ CR={cr_val:.1f}')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "7aGFxwOu8jR4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CR_list = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
        "SNR_list = [0, 6, 12, 18]\n",
        "\n",
        "importance_table = compute_feature_importance_table(\n",
        "    model=DeepJSCC_V,\n",
        "    test_loader=test_loader,\n",
        "    CHANNEL=CHANNEL,\n",
        "    Img_transform=Img_transform,\n",
        "    Compute_batch_PSNR=Compute_batch_PSNR,\n",
        "    CR_list=CR_list,\n",
        "    SNR_list=SNR_list\n",
        ")\n",
        "\n",
        "plot_importance_heatmap(importance_table, CR_list, SNR_list)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "WQQOmF0t9Lzw",
        "outputId": "5ffd5c2c-9619-4cda-9582-62b91d5c9b6f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'mu' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-88fe5822e7d1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mSNR_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m18\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m importance_table = compute_feature_importance_table(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDeepJSCC_V\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtest_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-4332d1a55ca1>\u001b[0m in \u001b[0;36mcompute_feature_importance_table\u001b[0;34m(model, test_loader, CHANNEL, Img_transform, Compute_batch_PSNR, CR_list, SNR_list)\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mcr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcr_val\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                 \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msnr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shape: [1, N]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m                 \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mimportance_table\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-b90c4b0a90b8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, snr)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;31m#snr = snr.view(-1, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAF1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msnr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAF2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msnr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-b90c4b0a90b8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, snr)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0msnr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msnr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [B, 1] 유지\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msnr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msnr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'mu' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_feature_importance_table(model, test_loader, CHANNEL, Img_transform, Compute_batch_PSNR,\n",
        "                                     CR_list=[0.1, 0.3, 0.5, 0.7, 0.9],\n",
        "                                     SNR_list=[0, 6, 12, 18]):\n",
        "    model.eval()\n",
        "    importance_table = None\n",
        "    start_time = time.time()\n",
        "\n",
        "    total_tasks = len(CR_list) * len(SNR_list)\n",
        "    tasks_completed = 0\n",
        "\n",
        "    for cr_idx, cr_val in enumerate(CR_list):\n",
        "        for snr_idx, snr_val in enumerate(SNR_list):\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            for test_input in test_loader:\n",
        "                test_input = test_input[:1].cuda()  # ✅ 단일 샘플만 사용\n",
        "                snr = snr_val * torch.ones((1, 1)).cuda()\n",
        "                cr = cr_val * torch.ones((1, 1)).cuda()\n",
        "\n",
        "                z = model.encoder(test_input, snr).detach()  # shape: [1, N]\n",
        "                N = z.shape[1]\n",
        "                if importance_table is None:\n",
        "                    importance_table = torch.zeros(len(CR_list), len(SNR_list), N).cuda()\n",
        "\n",
        "                full_mask = torch.ones_like(z).int()  # shape: [1, N]\n",
        "                z_masked = z * full_mask\n",
        "                z_channel = Channel_VLC(z_masked, snr, cr, CHANNEL)\n",
        "                rec_full = model.decoder(z_channel, snr).detach()\n",
        "                psnr_full = Compute_batch_PSNR(Img_transform(test_input), Img_transform(rec_full))\n",
        "\n",
        "                drop_scores = torch.zeros(N).cuda()\n",
        "                for f in range(N):\n",
        "                    iter_start = time.time()\n",
        "                    mask = full_mask.clone()\n",
        "                    mask[0, f] = 0  # ✅ 첫 샘플만 마스킹\n",
        "                    z_f = z * mask\n",
        "                    z_channel_f = Channel_VLC(z_f, snr, cr, CHANNEL)\n",
        "                    rec_f = model.decoder(z_channel_f, snr).detach()\n",
        "                    psnr_f = Compute_batch_PSNR(Img_transform(test_input), Img_transform(rec_f))\n",
        "                    drop_scores[f] = psnr_full - psnr_f\n",
        "\n",
        "                    del mask, z_f, z_channel_f, rec_f, psnr_f\n",
        "                    torch.cuda.empty_cache()\n",
        "                    gc.collect()\n",
        "\n",
        "                    feature_elapsed = time.time() - iter_start\n",
        "                    percent = (f + 1) / N * 100\n",
        "                    print(f\"    - Feature {f+1}/{N} ({percent:.1f}%) 완료 | 소요: {feature_elapsed:.1f}s\")\n",
        "\n",
        "                importance_table[cr_idx, snr_idx, :] = drop_scores\n",
        "\n",
        "                tasks_completed += 1\n",
        "                elapsed = time.time() - start_time\n",
        "                avg_per_task = elapsed / tasks_completed\n",
        "                remaining_tasks = total_tasks - tasks_completed\n",
        "                eta = remaining_tasks * avg_per_task\n",
        "                print(f\"[CR={cr_val:.1f}, SNR={snr_val}] 완료 | 경과: {elapsed:.1f}s | ETA: {eta:.1f}s\")\n",
        "\n",
        "                show_intermediate_heatmap(\n",
        "                    importance_table[:cr_idx+1, :, :].cpu().numpy(),\n",
        "                    CR_list[:cr_idx+1],\n",
        "                    SNR_list\n",
        "                )\n",
        "\n",
        "                break  # 한 샘플만 쓰고 다음 CR/SNR로 넘어감\n",
        "\n",
        "    return importance_table.cpu().numpy()\n"
      ],
      "metadata": {
        "id": "Yi43PgmPElLa"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}