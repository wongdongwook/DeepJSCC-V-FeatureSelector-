{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMX2wv11m5sBz3QrK/atj/Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wongdongwook/DeepJSCC-V-FeatureSelector-/blob/main/DeepJSSC_V_(Feature_Selector).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPf7WuhaYEg5"
      },
      "outputs": [],
      "source": [
        "#GDN\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "# from torchvision import datasets, transforms\n",
        "# from torchvision.utils import save_image\n",
        "from torch.autograd import Function\n",
        "\n",
        "\n",
        "class LowerBound(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, inputs, bound):\n",
        "        b = torch.ones_like(inputs) * bound\n",
        "        ctx.save_for_backward(inputs, b)\n",
        "        return torch.max(inputs, b)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        inputs, b = ctx.saved_tensors\n",
        "        pass_through_1 = inputs >= b\n",
        "        pass_through_2 = grad_output < 0\n",
        "\n",
        "        pass_through = pass_through_1 | pass_through_2\n",
        "        return pass_through.type(grad_output.dtype) * grad_output, None\n",
        "\n",
        "\n",
        "class GDN(nn.Module):\n",
        "    \"\"\"Generalized divisive normalization layer.\n",
        "    y[i] = x[i] / sqrt(beta[i] + sum_j(gamma[j, i] * x[j]))\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 ch,\n",
        "                 inverse=False,\n",
        "                 beta_min=1e-6,\n",
        "                 gamma_init=0.1,\n",
        "                 reparam_offset=2**-18):\n",
        "        super(GDN, self).__init__()\n",
        "        self.inverse = inverse\n",
        "        self.beta_min = beta_min\n",
        "        self.gamma_init = gamma_init\n",
        "        self.reparam_offset = reparam_offset\n",
        "\n",
        "        self.build(ch)\n",
        "\n",
        "    def build(self, ch):\n",
        "        self.pedestal = self.reparam_offset**2\n",
        "        self.beta_bound = ((self.beta_min + self.reparam_offset**2)**0.5)\n",
        "        self.gamma_bound = self.reparam_offset\n",
        "\n",
        "        # Create beta param\n",
        "        beta = torch.sqrt(torch.ones(ch)+self.pedestal)\n",
        "        self.beta = nn.Parameter(beta)\n",
        "\n",
        "        # Create gamma param\n",
        "        eye = torch.eye(ch)\n",
        "        g = self.gamma_init*eye\n",
        "        g = g + self.pedestal\n",
        "        gamma = torch.sqrt(g)\n",
        "\n",
        "        self.gamma = nn.Parameter(gamma)\n",
        "        self.pedestal = self.pedestal\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        unfold = False\n",
        "        if inputs.dim() == 5:\n",
        "            unfold = True\n",
        "            bs, ch, d, w, h = inputs.size()\n",
        "            inputs = inputs.view(bs, ch, d*w, h)\n",
        "\n",
        "        _, ch, _, _ = inputs.size()\n",
        "\n",
        "        # Beta bound and reparam\n",
        "        beta = LowerBound.apply(self.beta, self.beta_bound)\n",
        "        beta = beta**2 - self.pedestal\n",
        "\n",
        "        # Gamma bound and reparam\n",
        "        gamma = LowerBound.apply(self.gamma, self.gamma_bound)\n",
        "        gamma = gamma**2 - self.pedestal\n",
        "        gamma = gamma.view(ch, ch, 1, 1)\n",
        "\n",
        "        # Norm pool calc\n",
        "        norm_ = nn.functional.conv2d(inputs**2, gamma, beta)\n",
        "        norm_ = torch.sqrt(norm_)\n",
        "\n",
        "        # Apply norm\n",
        "        if self.inverse:\n",
        "            outputs = inputs * norm_\n",
        "        else:\n",
        "            outputs = inputs / norm_\n",
        "\n",
        "        if unfold:\n",
        "            outputs = outputs.view(bs, ch, d, w, h)\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# models\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "def conv(in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
        "    return nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n",
        "\n",
        "def deconv(in_channels, out_channels, kernel_size=3, stride=1, padding=1, output_padding = 0):\n",
        "    return nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, output_padding = output_padding,bias=False)\n",
        "\n",
        "\n",
        "class conv_block(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
        "        super(conv_block, self).__init__()\n",
        "        self.conv = conv(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "        self.gdn = nn.GDN(out_channels)\n",
        "        self.prelu = nn.PReLU()\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        out = self.gdn(out)\n",
        "        out = self.prelu(out)\n",
        "        return out\n",
        "\n",
        "class deconv_block(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, output_padding = 0):\n",
        "        super(deconv_block, self).__init__()\n",
        "        self.deconv = deconv(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding,  output_padding = output_padding)\n",
        "        self.gdn = nn.GDN(out_channels)\n",
        "        self.prelu = nn.PReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    def forward(self, x, activate_func='prelu'):\n",
        "        out = self.deconv(x)\n",
        "        out = self.gdn(out)\n",
        "        if activate_func=='prelu':\n",
        "            out = self.prelu(out)\n",
        "        elif activate_func=='sigmoid':\n",
        "            out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class conv_ResBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, use_conv1x1=False, kernel_size=3, stride=1, padding=1):\n",
        "        super(conv_ResBlock, self).__init__()\n",
        "        self.conv1 = conv(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "        self.conv2 = conv(out_channels, out_channels, kernel_size=1, stride = 1, padding=0)\n",
        "        self.gdn1 = GDN(out_channels)\n",
        "        self.gdn2 = GDN(out_channels)\n",
        "        self.prelu = nn.PReLU()\n",
        "        self.use_conv1x1 = use_conv1x1\n",
        "        if use_conv1x1 == True:\n",
        "            self.conv3 = conv(in_channels, out_channels, kernel_size=1, stride=stride, padding=0)\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.gdn1(out)\n",
        "        out = self.prelu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.gdn2(out)\n",
        "        if self.use_conv1x1 == True:\n",
        "            x = self.conv3(x)\n",
        "        out = out+x\n",
        "        out = self.prelu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class deconv_ResBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, use_deconv1x1=False, kernel_size=3, stride=1, padding=1, output_padding=0):\n",
        "        super(deconv_ResBlock, self).__init__()\n",
        "        self.deconv1 = deconv(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, output_padding=output_padding)\n",
        "        self.deconv2 = deconv(out_channels, out_channels, kernel_size=1, stride = 1, padding=0, output_padding=0)\n",
        "        self.gdn1 = GDN(out_channels)\n",
        "        self.gdn2 = GDN(out_channels)\n",
        "        self.prelu = nn.PReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.use_deconv1x1 = use_deconv1x1\n",
        "        if use_deconv1x1 == True:\n",
        "            self.deconv3 = deconv(in_channels, out_channels, kernel_size=1, stride=stride, padding=0, output_padding=output_padding)\n",
        "    def forward(self, x, activate_func='prelu'):\n",
        "        out = self.deconv1(x)\n",
        "        out = self.gdn1(out)\n",
        "        out = self.prelu(out)\n",
        "        out = self.deconv2(out)\n",
        "        out = self.gdn2(out)\n",
        "        if self.use_deconv1x1 == True:\n",
        "            x = self.deconv3(x)\n",
        "        out = out+x\n",
        "        if activate_func=='prelu':\n",
        "            out = self.prelu(out)\n",
        "        elif activate_func=='sigmoid':\n",
        "            out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "# Original Existing Works\n",
        "class AF_block(nn.Module):\n",
        "    def __init__(self, Nin, Nh, No):\n",
        "        super(AF_block, self).__init__()\n",
        "        self.fc1 = nn.Linear(Nin+1, Nh)\n",
        "        self.fc2 = nn.Linear(Nh, No)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    def forward(self, x, snr):\n",
        "        # out = F.adaptive_avg_pool2d(x, (1,1))\n",
        "        # out = torch.squeeze(out)\n",
        "        # out = torch.cat((out, snr), 1)\n",
        "        if snr.shape[0]>1:\n",
        "            snr = snr.squeeze()\n",
        "        snr = snr.unsqueeze(1)\n",
        "        mu = torch.mean(x, (2, 3))\n",
        "        out = torch.cat((mu, snr), 1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.sigmoid(out)\n",
        "        out = out.unsqueeze(2)\n",
        "        out = out.unsqueeze(3)\n",
        "        out = out*x\n",
        "        return out\n",
        "\n",
        "# The Encoder model with attention feature blocks\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, enc_shape, kernel_sz, Nc_conv):\n",
        "        super(Encoder, self).__init__()\n",
        "        enc_N = enc_shape[0]\n",
        "        Nh_AF = Nc_conv//2\n",
        "        padding_L = (kernel_sz-1)//2\n",
        "        self.conv1 = conv_ResBlock(3, Nc_conv, use_conv1x1=True, kernel_size = kernel_sz, stride = 2, padding=padding_L) # 데이터셋의 채널수에따라서 1이 3으로 바뀌고 3이 1로 바뀔 수 있음\n",
        "        self.conv2 = conv_ResBlock(Nc_conv, Nc_conv, use_conv1x1=True, kernel_size = kernel_sz, stride = 2, padding=padding_L)\n",
        "        self.conv3 = conv_ResBlock(Nc_conv, Nc_conv, kernel_size = kernel_sz, stride = 1, padding=padding_L)\n",
        "        self.conv4 = conv_ResBlock(Nc_conv, Nc_conv, kernel_size = kernel_sz, stride = 1, padding=padding_L)\n",
        "        self.conv5 = conv_ResBlock(Nc_conv, enc_N, use_conv1x1=True, kernel_size = kernel_sz, stride = 1, padding=padding_L)\n",
        "        self.AF1 = AF_block(Nc_conv, Nh_AF, Nc_conv)\n",
        "        self.AF2 = AF_block(Nc_conv, Nh_AF, Nc_conv)\n",
        "        self.AF3 = AF_block(Nc_conv, Nh_AF, Nc_conv)\n",
        "        self.AF4 = AF_block(Nc_conv, Nh_AF, Nc_conv)\n",
        "        self.AF5 = AF_block(enc_N, enc_N//2, enc_N)\n",
        "        self.flatten = nn.Flatten()\n",
        "    def forward(self, x, snr):\n",
        "        #snr = snr.view(-1, 1)\n",
        "        out = self.conv1(x)\n",
        "        out = self.AF1(out, snr)\n",
        "        out = self.conv2(out)\n",
        "        out = self.AF2(out, snr)\n",
        "        out = self.conv3(out)\n",
        "        out = self.AF3(out, snr)\n",
        "        out = self.conv4(out)\n",
        "        out = self.AF4(out, snr)\n",
        "        out = self.conv5(out)\n",
        "        out = self.AF5(out, snr)\n",
        "        out = self.flatten(out)\n",
        "        return out\n",
        "\n",
        "# The Decoder model with attention feature blocks\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, enc_shape, kernel_sz, Nc_deconv):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.enc_shape = enc_shape\n",
        "        Nh_AF1 = enc_shape[0]//2\n",
        "        Nh_AF = Nc_deconv//2\n",
        "        padding_L = (kernel_sz-1)//2\n",
        "        self.deconv1 = deconv_ResBlock(self.enc_shape[0], Nc_deconv, use_deconv1x1=True, kernel_size = kernel_sz, stride = 2,  padding=padding_L, output_padding = 1)\n",
        "        self.deconv2 = deconv_ResBlock(Nc_deconv, Nc_deconv, use_deconv1x1=True, kernel_size = kernel_sz, stride = 2,  padding=padding_L, output_padding = 1)\n",
        "        self.deconv3 = deconv_ResBlock(Nc_deconv, Nc_deconv, kernel_size=kernel_sz, stride=1, padding=padding_L)\n",
        "        self.deconv4 = deconv_ResBlock(Nc_deconv, Nc_deconv, kernel_size=kernel_sz, stride=1, padding=padding_L)\n",
        "        self.deconv5 = deconv_ResBlock(Nc_deconv, 3, use_deconv1x1=True, kernel_size=kernel_sz, stride=1, padding=padding_L)\n",
        "\n",
        "        self.AF1 = AF_block(self.enc_shape[0], Nh_AF1, self.enc_shape[0])\n",
        "        self.AF2 = AF_block(Nc_deconv, Nh_AF, Nc_deconv)\n",
        "        self.AF3 = AF_block(Nc_deconv, Nh_AF, Nc_deconv)\n",
        "        self.AF4 = AF_block(Nc_deconv, Nh_AF, Nc_deconv)\n",
        "        self.AF5 = AF_block(Nc_deconv, Nh_AF, Nc_deconv)\n",
        "    def forward(self, x, snr):\n",
        "        #snr = snr.view(-1, 1)\n",
        "        out = x.view(-1, self.enc_shape[0], self.enc_shape[1], self.enc_shape[2])\n",
        "        out = self.AF1(out, snr)\n",
        "        out = self.deconv1(out)\n",
        "        out = self.AF2(out, snr)\n",
        "        out = self.deconv2(out)\n",
        "        out = self.AF3(out, snr)\n",
        "        out = self.deconv3(out)\n",
        "        out = self.AF4(out, snr)\n",
        "        out = self.deconv4(out)\n",
        "        out = self.AF5(out, snr)\n",
        "        out = self.deconv5(out, 'sigmoid')\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Power normalization before transmission\n",
        "# Note: if P = 1, the symbol power is 2\n",
        "# If you want to set the average power as 1, please change P as P=1/np.sqrt(2)\n",
        "def Power_norm(z, P = 1):\n",
        "    batch_size, z_dim = z.shape\n",
        "    z_power = torch.sqrt(torch.sum(z**2, 1))\n",
        "    z_M = z_power.repeat(z_dim, 1)\n",
        "    return np.sqrt(P*z_dim)*z/z_M.t()\n",
        "\n",
        "def Power_norm_complex(z, P = 1):\n",
        "    batch_size, z_dim = z.shape\n",
        "    z_com = torch.complex(z[:, 0:z_dim:2], z[:, 1:z_dim:2])\n",
        "    z_com_conj = torch.complex(z[:, 0:z_dim:2], -z[:, 1:z_dim:2])\n",
        "    z_power = torch.sum(z_com*z_com_conj, 1).real\n",
        "    z_M = z_power.repeat(z_dim//2, 1)\n",
        "    z_nlz = np.sqrt(P*z_dim)*z_com/torch.sqrt(z_M.t())\n",
        "    z_out = torch.zeros(batch_size, z_dim).cuda()\n",
        "    z_out[:, 0:z_dim:2] = z_nlz.real\n",
        "    z_out[:, 1:z_dim:2] = z_nlz.imag\n",
        "    return z_out\n",
        "\n",
        "# The (real) AWGN channel\n",
        "def AWGN_channel(x, snr, P = 2):\n",
        "    batch_size, length = x.shape\n",
        "    gamma = 10 ** (snr / 10.0)\n",
        "    noise = torch.sqrt(P/gamma)*torch.randn(batch_size, length).cuda()\n",
        "    y = x+noise\n",
        "    return y\n",
        "\n",
        "def AWGN_complex(x, snr, Ps = 1):\n",
        "    batch_size, length = x.shape\n",
        "    gamma = 10 ** (snr / 10.0)\n",
        "    n_I = torch.sqrt(Ps/gamma)*torch.randn(batch_size, length).cuda()\n",
        "    n_R = torch.sqrt(Ps/gamma)*torch.randn(batch_size, length).cuda()\n",
        "    noise = torch.complex(n_I, n_R)\n",
        "    y = x + noise\n",
        "    return y\n",
        "\n",
        "# Please set the symbol power if it is not a default value\n",
        "def Fading_channel(x, snr, P = 2):\n",
        "    gamma = 10 ** (snr / 10.0)\n",
        "    [batch_size, feature_length] = x.shape\n",
        "    K = feature_length//2\n",
        "\n",
        "    h_I = torch.randn(batch_size, K).cuda()\n",
        "    h_R = torch.randn(batch_size, K).cuda()\n",
        "    h_com = torch.complex(h_I, h_R)\n",
        "    x_com = torch.complex(x[:, 0:feature_length:2], x[:, 1:feature_length:2])\n",
        "    y_com = h_com*x_com\n",
        "\n",
        "    n_I = torch.sqrt(P/gamma)*torch.randn(batch_size, K).cuda()\n",
        "    n_R = torch.sqrt(P/gamma)*torch.randn(batch_size, K).cuda()\n",
        "    noise = torch.complex(n_I, n_R)\n",
        "\n",
        "    y_add = y_com + noise\n",
        "    y = y_add/h_com\n",
        "\n",
        "    y_out = torch.zeros(batch_size, feature_length).cuda()\n",
        "    y_out[:, 0:feature_length:2] = y.real\n",
        "    y_out[:, 1:feature_length:2] = y.imag\n",
        "    return y_out\n",
        "\n",
        "\n",
        "\n",
        "# Note: if P = 1, the symbol power is 2\n",
        "# If you want to set the average power as 1, please change P as P=1/np.sqrt(2)\n",
        "def Power_norm_VLC(z, cr, P = 1):\n",
        "    batch_size, z_dim = z.shape\n",
        "    Kv = torch.ceil(z_dim*cr).int()\n",
        "    z_power = torch.sqrt(torch.sum(z**2, 1))\n",
        "    z_M = z_power.repeat(z_dim, 1).cuda()\n",
        "    return torch.sqrt(Kv*P)*z/z_M.t()\n",
        "\n",
        "\n",
        "def AWGN_channel_VLC(x, snr, cr, P = 2):\n",
        "    batch_size, length = x.shape\n",
        "    gamma = 10 ** (snr / 10.0)\n",
        "    mask = mask_gen(length, cr).cuda()\n",
        "    noise = torch.sqrt(P/gamma)*torch.randn(1, length).cuda()\n",
        "    noise = noise*mask\n",
        "    y = x+noise\n",
        "    return y\n",
        "\n",
        "\n",
        "def Fading_channel_VLC(x, snr, cr, P = 2):\n",
        "    gamma = 10 ** (snr / 10.0)\n",
        "    [batch_size, feature_length] = x.shape\n",
        "    K = feature_length//2\n",
        "\n",
        "    mask = mask_gen(K, cr).cuda()\n",
        "    h_I = torch.randn(batch_size, K).cuda()\n",
        "    h_R = torch.randn(batch_size, K).cuda()\n",
        "    h_com = torch.complex(h_I, h_R)\n",
        "    x_com = torch.complex(x[:, 0:feature_length:2], x[:, 1:feature_length:2])\n",
        "    y_com = h_com*x_com\n",
        "\n",
        "    n_I = torch.sqrt(P/gamma)*torch.randn(batch_size, K).cuda()\n",
        "    n_R = torch.sqrt(P/gamma)*torch.randn(batch_size, K).cuda()\n",
        "    noise = torch.complex(n_I, n_R)*mask\n",
        "\n",
        "    y_add = y_com + noise\n",
        "    y = y_add/h_com\n",
        "\n",
        "    y_out = torch.zeros(batch_size, feature_length).cuda()\n",
        "    y_out[:, 0:feature_length:2] = y.real\n",
        "    y_out[:, 1:feature_length:2] = y.imag\n",
        "    return y_out\n",
        "\n",
        "\n",
        "def Channel(z, snr, channel_type = 'AWGN'):\n",
        "    z = Power_norm(z)\n",
        "    if channel_type == 'AWGN':\n",
        "        z = AWGN_channel(z, snr)\n",
        "    elif channel_type == 'Fading':\n",
        "        z = Fading_channel(z, snr)\n",
        "    return z\n",
        "\n",
        "\n",
        "def Channel_VLC(z, snr, cr, channel_type = 'AWGN'):\n",
        "    z = Power_norm_VLC(z, cr)\n",
        "    if channel_type == 'AWGN':\n",
        "        z = AWGN_channel_VLC(z, snr, cr)\n",
        "    elif channel_type == 'Fading':\n",
        "        z = Fading_channel_VLC(z, snr, cr)\n",
        "    return z\n",
        "\n",
        "\n",
        "def mask_gen(N, cr, ch_max = 48):\n",
        "    MASK = torch.zeros(cr.shape[0], N).int()\n",
        "    nc = N//ch_max\n",
        "    for i in range(0, cr.shape[0]):\n",
        "        L_i = nc*torch.round(ch_max*cr[i]).int()\n",
        "        MASK[i, 0:L_i] = 1\n",
        "    return MASK\n",
        "\n",
        "\n",
        "class ADJSCC(nn.Module):\n",
        "    def __init__(self, enc_shape, Kernel_sz, Nc):\n",
        "        super(ADJSCC, self).__init__()\n",
        "        self.encoder = Encoder(enc_shape, Kernel_sz, Nc)\n",
        "        self.decoder = Decoder(enc_shape, Kernel_sz, Nc)\n",
        "    def forward(self, x, snr, channel_type = 'AWGN'):\n",
        "        z = self.encoder(x, snr)\n",
        "        z = Channel(z, snr, channel_type)\n",
        "        out = self.decoder(z, snr)\n",
        "        return out\n",
        "\n",
        "# The DeepJSCC_V model, also called ADJSCC_V\n",
        "class ADJSCC_V(nn.Module):\n",
        "    def __init__(self, enc_shape, Kernel_sz, Nc):\n",
        "        super(ADJSCC_V, self).__init__()\n",
        "        self.encoder = Encoder(enc_shape, Kernel_sz, Nc)\n",
        "        self.decoder = Decoder(enc_shape, Kernel_sz, Nc)\n",
        "    def forward(self, x, snr, cr, channel_type = 'AWGN'):\n",
        "        z = self.encoder(x, snr)\n",
        "        z = z*mask_gen(z.shape[1], cr).cuda()\n",
        "        z = Channel_VLC(z, snr, cr, channel_type)\n",
        "        out = self.decoder(z, snr)\n",
        "        return out"
      ],
      "metadata": {
        "id": "uC5dXz8ZYihk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import cifar100\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# CIFAR-100 Data Loading Function\n",
        "def load_cifar100_data():\n",
        "    (x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
        "    x_train = np.transpose(x_train, (0, 3, 1, 2))  # Dimension Rearragentment: [batch size, channel, height, width]\n",
        "    x_test = np.transpose(x_test, (0, 3, 1, 2))    # Dimension Rearragentment\n",
        "    x_train = x_train.astype('float32') / 255\n",
        "    x_test = x_test.astype('float32') / 255\n",
        "    return x_train, x_test\n",
        "\n",
        "# Data Loader 클래스\n",
        "class DatasetFolder(Dataset):\n",
        "    def __init__(self, matData):\n",
        "        self.matdata = matData\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.matdata[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.matdata.shape[0]"
      ],
      "metadata": {
        "id": "M2v13rJ8Yj9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 400\n",
        "LEARNING_RATE = 1e-4\n",
        "#LEARNING_RATE = 3e-4\n",
        "PRINT_RREQ = 150\n",
        "\n",
        "CHANNEL = 'AWGN'  # Choose AWGN or Fading\n",
        "IMG_SIZE = [3, 32, 32]  # CIFAR-100 Image shape\n",
        "N_channels = 256\n",
        "Kernel_sz = 5\n",
        "\n",
        "x_train, x_test = load_cifar100_data()\n",
        "\n",
        "train_dataset = DatasetFolder(x_train)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
        "test_dataset = DatasetFolder(x_test)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "current_epoch = 0\n",
        "CONTINUE_TRAINING = False\n",
        "\n",
        "KSZ = str(Kernel_sz)+'x'+str(Kernel_sz)+'_'\n"
      ],
      "metadata": {
        "id": "989kJeJZYkAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test Metric (PSNR)\n",
        "from skimage.metrics import peak_signal_noise_ratio as compute_pnsr\n",
        "\n",
        "\n",
        "def Img_transform(test_rec):\n",
        "    test_rec = test_rec.permute(0, 2, 3, 1)\n",
        "    test_rec = test_rec.cpu().detach().numpy()\n",
        "    test_rec = test_rec*255\n",
        "    test_rec = test_rec.astype(np.uint8)\n",
        "    return test_rec\n",
        "\n",
        "def Compute_batch_PSNR(test_input, test_rec):\n",
        "    psnr_i1 = np.zeros((test_input.shape[0]))\n",
        "    for j in range(0, test_input.shape[0]):\n",
        "        psnr_i1[j] = compute_pnsr(test_input[j, :], test_rec[j, :])\n",
        "    psnr_ave = np.mean(psnr_i1)\n",
        "    return psnr_ave\n",
        "\n",
        "\n",
        "def Compute_IMG_PSNR(test_input, test_rec):\n",
        "    psnr_i1 = np.zeros((test_input.shape[0], 1))\n",
        "    for j in range(0, test_input.shape[0]):\n",
        "        psnr_i1[j] = compute_pnsr(test_input[j, :], test_rec[j, :])\n",
        "    return psnr_i1\n"
      ],
      "metadata": {
        "id": "-eGPDfqSYmu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LEARNING_RATE = 1e-4\n",
        "CONTINUE_TRAINING= False\n",
        "enc_out_shape = [48, IMG_SIZE[1]//4, IMG_SIZE[2]//4] #가장 첫번째가 48,  오른쪽이 feature map // ADJSCC인 경우 수정되어야함\n",
        "\n",
        "DeepJSCC_V = ADJSCC_V(enc_out_shape, Kernel_sz, N_channels).cuda()\n",
        "criterion = nn.MSELoss().cuda()\n",
        "optimizer = torch.optim.Adam(DeepJSCC_V.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "current_epoch = 0\n",
        "bestLoss = 1e3\n",
        "if CONTINUE_TRAINING == True:\n",
        "    #DeepJSCC_V.load_state_dict(torch.load('./JSCC_models/DeepJSCC_VLC_'+KSZ+CHANNEL+'_'+str(N_channels)+'_201.pth.tar')['state_dict'])\n",
        "    #current_epoch = 204\n",
        "    current_epoch = 10\n",
        "    DeepJSCC_V.load_state_dict(torch.load('/content/JSCC_models/DeepJSCC__5x5_AWGN_256_9.pth.tar')['state_dict'])\n"
      ],
      "metadata": {
        "id": "XZ4K8AcpYoIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DeepJSCC_V + Exisitng Attention\n",
        "print('Training for DeepJSCC_V is started!')\n",
        "bestLoss = 1e3\n",
        "\n",
        "current_epoch=0\n",
        "EPOCHS= 101\n",
        "\n",
        "for epoch in range(current_epoch, EPOCHS):\n",
        "    DeepJSCC_V.train()\n",
        "    print('========================')\n",
        "    print('lr:%.4e'%optimizer.param_groups[0]['lr'])\n",
        "\n",
        "    # Model training\n",
        "    for i, x_input in enumerate(train_loader):\n",
        "        x_input = x_input.cuda()\n",
        "\n",
        "        SNR_TRAIN = torch.randint(0, 28, (x_input.shape[0], 1)).cuda()\n",
        "        CR = 0.1+0.9*torch.rand(x_input.shape[0], 1).cuda()\n",
        "        x_rec = DeepJSCC_V(x_input, SNR_TRAIN, CR, CHANNEL)\n",
        "        loss = criterion(x_input, x_rec)\n",
        "        loss = loss.mean()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if i % PRINT_RREQ == 0:\n",
        "            print('Epoch: [{0}][{1}/{2}]\\t' 'Loss {loss:.4f}\\t'.format(epoch, i, len(train_loader), loss=loss.item()))\n",
        "\n",
        "    # Model Evaluation\n",
        "    DeepJSCC_V.eval()\n",
        "    totalLoss = 0\n",
        "    with torch.no_grad():\n",
        "        for i, test_input in enumerate(test_loader):\n",
        "            test_input = test_input.cuda()\n",
        "            SNR_TEST = torch.randint(0, 28, (test_input.shape[0], 1)).cuda()\n",
        "            CR = 0.1+0.9*torch.rand(test_input.shape[0], 1).cuda()\n",
        "            test_rec = DeepJSCC_V(test_input, SNR_TEST, CR, CHANNEL)\n",
        "            totalLoss += criterion(test_rec, test_input).item() * test_input.size(0)\n",
        "        averageLoss = totalLoss / (len(test_dataset))\n",
        "        print('averageLoss=', averageLoss)\n",
        "        if averageLoss < bestLoss:\n",
        "            # Model saving\n",
        "            if not os.path.exists('./JSCC_models'):\n",
        "                os.makedirs('./JSCC_models')\n",
        "            torch.save({'state_dict': DeepJSCC_V.state_dict(), }, './JSCC_models/DeepJSCC_'+KSZ+CHANNEL+'_'+str(N_channels)+'_'+str(epoch)+'.pth.tar')\n",
        "\n",
        "            print('Model saved')\n",
        "            bestLoss = averageLoss\n",
        "\n",
        "print('Training for DeepJSCC_V is finished!')\n"
      ],
      "metadata": {
        "id": "ftoKeTRoYpHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DeepJSCC-V + Exisitng Attention\n",
        "kernel_sz = 5\n",
        "KSZ = '_'+str(kernel_sz)+'x'+str(kernel_sz)+'_'\n",
        "PSNR_ave = np.zeros((10, 10))\n",
        "CR=0.3 # temporary test CR\n",
        "\n",
        "for m in range(0, 10):\n",
        "    # enc_shape = [96//CR_INDEX[m], 8, 8]\n",
        "    #DeepJSCC_V = ADJSCC_V(enc_out_shape, kernel_sz, N_channels).cuda()\n",
        "    # DeepJSCC = nn.DataParallel(DeepJSCC)\n",
        "    cr= 1/3\n",
        "\n",
        "    for k in range(0, 10):\n",
        "        print('Evaluating DeepJSCC-v with CR = '+str(cr)+' and SNR = '+str(3*k-3)+'dB')\n",
        "        total_psnr = 0\n",
        "        DeepJSCC_V.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, test_input in enumerate(test_loader):\n",
        "                SNR = 3*(k-1)*torch.ones((test_input.shape[0], 1)).cuda()\n",
        "                CR = cr*torch.ones((test_input.shape[0], 1)).cuda()\n",
        "                test_input = test_input.cuda()\n",
        "\n",
        "                test_rec = DeepJSCC_V(test_input, SNR,CR, CHANNEL)\n",
        "\n",
        "                test_input = Img_transform(test_input)\n",
        "                test_rec  = Img_transform(test_rec)\n",
        "                psnr_ave = Compute_batch_PSNR(test_input, test_rec)\n",
        "                total_psnr += psnr_ave\n",
        "            averagePSNR = total_psnr / i\n",
        "            print('PSNR = ' + str(averagePSNR))\n",
        "\n",
        "        PSNR_ave[m, k] = averagePSNR"
      ],
      "metadata": {
        "id": "W3bkKDabYqK_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}